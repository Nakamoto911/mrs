# Asset-Specific Macro Regime Detection System Configuration
# Version 1.0 | January 27, 2026

# =============================================================================
# Data Configuration
# =============================================================================
data:
  # FRED-MD settings
  fred_md:
    url: "https://files.stlouisfed.org/files/htdocs/fred-md/monthly/current.csv"
    transform_url: "https://files.stlouisfed.org/files/htdocs/fred-md/Appendix_Tables_Update.pdf"
    n_variables: 128
    # NEW: Explicitly exclude stock market variables (Category 8)
    exclude_categories: [6, 8]  # 6 = Interest and Exchange Rates, 8 = Stock Market
    exclude_variables: []    # Optional: specific variables to drop
    categories:
      - name: "Output/Income"
        count: 17
      - name: "Labor"
        count: 32
      - name: "Housing"
        count: 10
      - name: "Consumption/Orders"
        count: 14
      - name: "Money/Credit"
        count: 14
      - name: "Interest Rates/Spreads"
        count: 22
      - name: "Prices"
        count: 21
      - name: "Stock Market"
        count: 3

  # Asset configuration
  assets:
    SPX:
      name: "S&P 500"
      modern_ticker: "SPY"
      modern_start: "1993-01-01"
      historical_proxy: "SP500"
      historical_start: "1959-01-01"
    
    BOND:
      name: "10Y Treasury Bond"
      modern_ticker: "IEF"
      modern_start: "2002-07-01"
      historical_proxy: "GS10"
      historical_start: "1959-01-01"
    
    GOLD:
      name: "Gold"
      modern_ticker: "GLD"
      modern_start: "2004-11-01"
      historical_proxy: "GOLDAMGBD228NLBM"
      historical_start: "1968-04-01"
      cpi_proxy_start: "1959-01-01"

  # ALFRED validation
  alfred:
    start_year: 2000
    end_year: 2024
    frequency: "semi-annual"  # 48 rebalancing periods
    publication_lag_months: 1
    api_base: "https://api.stlouisfed.org/fred"

# =============================================================================
# Feature Engineering Configuration
# =============================================================================
features:
  # Stationarity tests
  stationarity:
    adf_significance: 0.05
    kpss_significance: 0.05
  
  # FRED-MD transformation codes
  transform_codes:
    1: "no_transform"       # Stationary
    2: "first_diff"         # Δx
    5: "log_diff"           # Δlog(x) - Growth rate
    6: "second_log_diff"    # Δ²log(x) - Acceleration

  # Macro ratios to generate
  ratios:
    liquidity:
      - ["M2SL", "GDP", "M2/GDP"]
      - ["M2SL", "PI", "M2/Personal_Income"]
      - ["BOGMBASE", "GDP", "Reserves/GDP"]
    leverage:
      - ["TCMDO", "GDP", "Total_Debt/GDP"]
      - ["TOTALSL", "DSPIC96", "Consumer_Debt/DPI"]
    real:
      - ["GS10", "CPIAUCSL", "Real_10Y_Yield"]
      - ["FEDFUNDS", "CPIAUCSL", "Real_Fed_Funds"]
    valuation:
      - ["SP500", "SP500_DIV", "PE_Ratio"]

  # Quintile features
  quintiles:
    # CHANGED: Set to "all" to auto-generate for every variable in the dataset
    variables: "all" 
    n_quintiles: 5
    encoding: "one_hot"  # or "z_score_within_regime"
    min_observations: 60
    monotonicity_threshold: 0.8

  # Cointegration pairs (theory-driven)
  cointegration:
    # Enable/disable validation
    validate: true
    significance_level: 0.05
    min_observations: 120
    
    # Stability requirements
    stability_threshold: 0.70  # 70% of rolling windows must show cointegration
    
    # Bayesian Prior Weighting (NEW Spec 03)
    priors:
      enabled: true
      evidence_method: "logistic"
      logistic_k: 10.0
      logistic_p0: 0.10
      stability_enabled: true
      min_weight_threshold: 0.3
      pair_priors:
        consumption_income: 0.8
        fisher_hypothesis: 0.6
        yields_inflation: 0.6
        quantity_theory: 0.5
        okun_law: 0.4
        housing_rates: 0.3
        stocks_gdp: 0.2
    
    # Pairs to test (theory column is informational)
    pairs:
      - ["INDPRO", "M2SL", "quantity_theory", "M2 velocity stability"]
      - ["GS10", "CPIAUCSL", "fisher_hypothesis", "Real rate stationarity"]
      - ["INDPRO", "PAYEMS", "okun_law", "Output-employment linkage"]
      - ["HOUST", "GS10", "housing_rates", "Housing demand elasticity"]
      - ["S&P 500", "S&P div yield", "gordon_growth", "Present value relation"]
      - ["DPCERA3M086SBEA", "RPI", "consumption_income", "Permanent income hypothesis"]
    
    # Maximum lag for ECT generation
    johansen_spec: "ci"
    max_lag: 12

  # Momentum windows (months)
  momentum_windows: [3, 6, 12]

  # Cross-asset features
  cross_asset:
    correlation_windows: [6, 12]
    pairs:
      - ["SPX", "BOND"]
      - ["SPX", "GOLD"]
      - ["BOND", "GOLD"]

  # Hierarchical clustering
  clustering:
    # UPDATED: Force exactly 20 orthogonal economic drivers
    n_clusters: 20
    similarity_threshold: null
    linkage_method: "average"   # Keep "average" to encourage spherical clusters
    distance_metric: "correlation"
    # Feature selection within clusters
    selection:
      # STRICTLY UNSUPERVISED: Select the "Medoid" (feature closest to cluster center)
      method: "centroid" 
      
      # DISABLE all target-aware selection (No Snooping)
      ic_based:
        enabled: false 
        lag_buffer_months: 24
        min_observations: 60

# =============================================================================
# Inference Configuration
# =============================================================================
inference:
  effective_sample_size:
    method: "newey_west"  # "simple", "newey_west", "hansen_hodrick", "autocorr"
    
    # Newey-West settings
    newey_west:
      bandwidth_method: "auto"  # "auto" (Andrews), "fixed", or integer
      fixed_bandwidth: null
      kernel: "bartlett"  # "bartlett", "parzen"
    
    # Autocorrelation-based settings
    autocorr:
      max_lags: 36  # Max lags to consider
      significance_threshold: 0.05
      
    # Reporting
    report_all_methods: true  # Compare all methods in output

# =============================================================================
# Model Configuration
# =============================================================================
models:
  # Target variables
  targets:
    returns:
      horizon_months: 12        # CHANGED: 12 months is the "Goldilocks" horizon
      type: "excess"            # NEW: "nominal", "excess", or "real"
      deflator: "GS10"          # NEW: FRED series ID for adjustment (GS10 for excess, CPIAUCSL for real)
      metric: "IC"
    volatility:
      horizon_months: 12        # CHANGED: Match return horizon
      lookback_months: 6        # Volatility lookback window
      metric: "RMSE"

  # Linear models
  linear:
    ridge:
      enabled: true
      alpha_range: [0.001, 100]
      params:
        alpha: 1.0
    lasso:
      enabled: true
      alpha_range: [0.001, 10]
      params:
        alpha: 0.0001
        max_iter: 100000
    elastic_net:
      enabled: true
      alpha_range: [0.001, 10]
      l1_ratio_range: [0.1, 0.9]
      params:
        alpha: 0.0001
        l1_ratio: 0.5
        max_iter: 100000
    vecm:
      enabled: false
      max_lag: 12
      coint_rank: "auto"

  # Tree-based models
  tree:
    random_forest:
      enabled: true
      n_estimators_range: [100, 500]
      max_depth_range: [3, 15]
      min_samples_leaf_range: [10, 50]
      params:
        n_estimators: 100
        max_depth: 3
        min_samples_leaf: 10
        n_jobs: -1
    xgboost:
      enabled: true
      learning_rate_range: [0.01, 0.3]
      max_depth_range: [3, 10]
      n_estimators_range: [100, 500]
      min_child_weight_range: [1, 10]
      subsample_range: [0.6, 1.0]
      colsample_bytree_range: [0.6, 1.0]
      params:
        n_estimators: 100
        max_depth: 3
        learning_rate: 0.05
        min_child_weight: 10
        subsample: 0.6
        colsample_bytree: 0.6
        n_jobs: -1
    lightgbm:
      enabled: false
      learning_rate_range: [0.01, 0.3]
      max_depth_range: [3, 12]
      n_estimators_range: [100, 500]
      num_leaves_range: [20, 100]
      params:
        n_estimators: 100
        max_depth: 4
        learning_rate: 0.1
        n_jobs: 1
    catboost:
      enabled: false
      learning_rate_range: [0.01, 0.3]
      depth_range: [4, 10]
      iterations_range: [100, 500]
      params:
        n_estimators: 100
        max_depth: 4
        learning_rate: 0.1

  # Neural networks
  neural:
    mlp:
      enabled: false
      hidden_layers_range: [1, 3]
      hidden_units_range: [32, 256]
      dropout_range: [0.1, 0.5]
      learning_rate_range: [0.0001, 0.01]
      params:
        hidden_layers: [64, 32]
        epochs: 50
    lstm:
      enabled: false
      sequence:
        length: 12  # Months per sequence
        stride: 1   # Step between sequences (1 = full overlap)
      min_requirements:
        min_total_sequences: 50    # Minimum across all data
        min_train_sequences: 30    # Minimum in training fold
        min_val_sequences: 10      # Minimum in validation fold
        safety_margin_pct: 0.10    # 10% buffer for NaN losses
      params: {}
      hidden_size: 32       # Reduced from 64
      n_layers: 1           # Reduced from 2
      use_attention: true   # Temporal attention
      dropout: 0.3          # Increased from 0.2
      weight_decay: 0.01    # L2 regularization
      input_noise_std: 0.1  # Noise injection
      learning_rate: 0.001
      epochs: 100
      batch_size: 16        # Smaller batches
      patience: 15          # Early stopping
      min_epochs: 20
      mc_dropout_samples: 50
    tcn:
      enabled: false  # Optional
      kernel_size_range: [2, 5]
      n_filters_range: [32, 128]

# =============================================================================
# Regime Detection Configuration
# =============================================================================
regimes:
  # Statistical Jump Model parameters
  jump_model:
    n_states: 2  # bullish, bearish
    features: ["return", "volatility"]
    volatility_lookback: 6  # months for realized vol
    em_iterations: 100
    convergence_threshold: 0.0001

  # Regime forecasting
  forecasting:
    model: "lightgbm"
    horizon_months: 6
    probability_threshold: 0.5

# =============================================================================
# Validation Configuration
# =============================================================================
validation:
  # Time-series cross-validation
  cv:
    min_train_months: 120  # 10 years minimum
    validation_months: 60  # Increased to 60 months to exceed 6M horizon (10x)
    step_months: 12  # annual rebalancing to match horizon
    n_folds: "auto"  # depends on data length

  # NEW: Holdout configuration (Added per Spec 01)
  holdout:
    enabled: true
    method: "percentage"
    holdout_pct: 0.15  # Last 15% of data
    min_holdout_months: 48
    min_independent_obs: 20

  # Hyperparameter optimization
  optuna:
    n_trials: 100
    timeout_hours: null
    sampler: "TPE"
    pruner: "MedianPruner"
    n_startup_trials: 10
    n_warmup_steps: 5

  # ALFRED validation
  alfred:
    deployment_criteria:
      ic_realtime_min: 0.15
      revision_risk_max: 0.30
      feature_stability_min: 0.70

# =============================================================================
# Evaluation Metrics
# =============================================================================
metrics:
  primary:
    returns: "IC"  # Information Coefficient (Spearman rank correlation)
    volatility: "RMSE"
  secondary:
    - "MAE"
    - "R2_OOS"
    - "hit_rate"
    - "directional_mae"
  
  # NEW: Inference configuration
  inference:
    method: "newey_west"  # newey_west, hansen_hodrick, bootstrap
    kernel: "bartlett"     # bartlett, parzen, quadratic
    bootstrap_reps: 1000
    alpha: 0.05           # Significance level
    
  # NEW: Robustness Suite (Spec 11)
  robustness:
    placebo_shuffles: 1000
    subsample_splits: 4
    economic_significance:
      costs_bps: 10
      vol_target: 0.10
    
  # =============================================================================
  # Performance Thresholds (Calibrated to Academic Literature)
  # =============================================================================
  thresholds:
    # Asset-specific IC thresholds (out-of-sample, adjusted for overlapping obs)
    # These reflect REALISTIC expectations, not aspirational targets
    
    equities:  # SPX
      ic_excellent: 0.12      # Top decile of published research
      ic_good: 0.08           # Economically meaningful
      ic_acceptable: 0.05     # Statistically detectable
      ic_minimum: 0.03        # Marginally useful
      ic_suspicious: 0.20     # Likely data issue or overfitting
      
    bonds:  # BOND
      ic_excellent: 0.18      # Bonds more predictable via term structure
      ic_good: 0.12
      ic_acceptable: 0.08
      ic_minimum: 0.05
      ic_suspicious: 0.30
      
    commodities:  # GOLD
      ic_excellent: 0.10      # Commodities least predictable
      ic_good: 0.06
      ic_acceptable: 0.04
      ic_minimum: 0.02
      ic_suspicious: 0.18
    
    # Statistical significance requirements
    inference:
      t_stat_minimum: 1.96    # 5% significance level
      p_value_maximum: 0.05
      effective_n_minimum: 15  # Minimum effective observations
      
    # Sanity checks
    anomaly_detection:
      max_in_sample_vs_oos_ratio: 2.0  # IS IC / OOS IC < 2
      max_ic_std_ratio: 3.0            # IC_mean / IC_std > 3 is suspicious
      
    # Economic significance
    economic:
      min_implied_sharpe: 0.20         # IC * sqrt(12 * turnover_adj)
      max_turnover_annual: 12          # Trades per year

# =============================================================================
# Performance Benchmarks (Expected)
# =============================================================================
benchmarks:
  SPX:
    ic_revised: [0.20, 0.28]
    ic_realtime: [0.15, 0.22]
    revision_risk: [0.15, 0.25]
  BOND:
    ic_revised: [0.28, 0.35]
    ic_realtime: [0.24, 0.30]
    revision_risk: [0.10, 0.18]
  GOLD:
    ic_revised: [0.12, 0.20]
    ic_realtime: [0.10, 0.16]
    revision_risk: [0.15, 0.22]

# =============================================================================
# Execution Configuration
# =============================================================================
execution:
  # Parallel processing
  n_jobs: -1  # Use all available cores
  backend: "loky"  # joblib backend
  
  # GPU settings (for neural networks)
  use_gpu: false
  gpu_device: 0
  
  # Memory management
  chunk_size: 10000
  cache_results: true
  
  # Logging
  log_level: "INFO"
  log_file: "experiments/logs/experiment.log"

# =============================================================================
# Output Configuration
# =============================================================================
output:
  # File formats
  features: "parquet"
  models: "joblib"
  results: "json"
  shap: "npy"
  reports: "excel"
  
  # Paths (relative to project root)
  paths:
    features: "experiments/features"
    models: "experiments/models"
    cv_results: "experiments/cv_results"
    shap: "experiments/shap"
    regimes: "experiments/regimes"
    reports: "experiments/reports"
    predictions: "experiments/predictions"

# =============================================================================
# Ensemble Strategy Configuration
# =============================================================================
ensemble:
  enabled: true
  size: 5
  
  # Selection Strategy
  selection:
    method: "nested_cv"  # "simple" (top-N of current CV), "nested_cv", or "fixed"
    
    nested_cv:
      n_outer_folds: 5
      n_inner_folds: 4
      outer_min_train_months: 120
      inner_min_train_months: 84
    
    fixed:
      models: ["ridge", "xgboost", "lightgbm"]

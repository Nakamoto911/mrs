# Technical Appendix C: Implementation & Deployment Guide

**Document 4 of 4** | **Version 1.0** | **January 27, 2026**

---

## Contents

1. [Modular Execution Framework](#modular-execution-framework)
2. [Streamlit User Interface Specifications](#streamlit-user-interface-specifications)
3. [Code Organization & Architecture](#code-organization--architecture)
4. [Computational Timing & Optimization](#computational-timing--optimization)
5. [Deployment Workflow & Checklist](#deployment-workflow--checklist)
6. [Final Deliverables Checklist](#final-deliverables-checklist)

---

## Modular Execution Framework

### 1.1 Execution Modes

| Mode | Command Example | Time (Sequential) | Use Case |
|---|---|---|---|
| Full Tournament | `--assets all --models all` | 6â€“9 hours | Initial discovery |
| Single Asset | `--asset SPX` | 2â€“3 hours | Asset-specific iteration |
| Single Model | `--model xgboost` | 1.5â€“2.5 hours | Model-specific tuning |
| Single Asset-Model | `--asset SPX --model lstm` | 20â€“40 min | Quick experiments |
| Features Only | `--features-only` | 1.5â€“2 hours | Debug pipeline |
| Evaluation Only | `--eval-only` | 15â€“30 min | Re-compute metrics |

### 1.2 Result Caching Strategy

- **Feature matrices:** Parquet files (`experiments/features/YYYYMMDD.parquet`)
- **Trained models:** Joblib/pickle (`experiments/models/SPX_xgboost_YYYYMMDD.pkl`)
- **CV results:** JSON (`experiments/cv_results/SPX_xgboost_cv.json`)
- **SHAP values:** NumPy arrays (`experiments/shap/SPX_xgboost_shap.npy`)
- **Hyperparameter history:** Optuna SQLite database
- **Regime labels:** CSV (`experiments/regimes/SPX_regime_labels.csv`)
- **Ensemble Manifest:** JSON (`experiments/models/SPX_Ensemble_Top5.json`)
- **OOS Predictions:** CSV (`experiments/predictions/SPX_Ridge_preds.csv`)

### 1.3 Parallel Execution

- **Asset-level:** Run 3 assets on different CPU cores (reduces wall time 60%)
- **Model-level:** Train multiple models in parallel within asset
- **CV-fold:** joblib/multiprocessing for cross-validation
- **Hyperparameter search:** Optuna supports concurrent trials

**Example:** 16-core machine â†’ 3 assets Ã— 3 cores each, 7 cores for hyperparam search

---

## Streamlit User Interface Specifications

### 2.1 Why Streamlit

| Framework | Pros | Cons | Verdict |
|---|---|---|---|
| Streamlit | Pure Python, rapid prototyping, auto-reload | Less customization | âœ“ **RECOMMENDED** |
| Dash | More control, production-ready | Steeper learning curve | Alternative |
| Gradio | Great for model demos | Too focused on inference | Not ideal |
| Flask/React | Full control, scalable | Requires frontend dev | Overkill |

### 2.2 Five-Page Application

**Page 1: Experiment Configuration**

Multi-select assets, checkboxes for models, radio buttons for target variable, sliders for hyperparameter budget, "Launch Experiment" button with progress bar

**Page 2: Model Comparison Dashboard**

Dropdown to select asset, performance metrics table (IC, RMSE, RÂ², Hit Rate) with **"Champion" Ensemble Highlighting**, heatmap (models Ã— metrics color-coded), **"ðŸ§© Ensemble Composition" expander** showing contribuing models, time-series of OOS performance, forecast vs. realized scatter plot, export to Excel

**Page 3: Feature Importance Explorer**

Top 20 features bar chart (SHAP values), tabs for Overall/Bullish/Bearish regimes, sortable table of all features, time-series feature stability plot, waterfall chart for specific predictions, export CSV

**Page 4: Dominant Drivers Monitoring**

Real-time monitoring sheet (rank, feature, SHAP, current percentile, signal) with **Ensemble Consensus mode**, alert indicators for extreme ranges (>95th or <5th percentile), regime summary (Bullish/Bearish/Neutral), **"Vote Split" progress bar** (consensus%), historical regime timeline

**Page 5: Data & Feature Inspector**

Upload custom CSV option, feature matrix preview (first/last 10 rows), correlation heatmap, missing data report (PIT availability scores), stationarity test results, variance filtering logs, and **PIT pruning logs** (tracking automated feature selection per expansion window to satisfy degree-of-freedom constraints).

---

## Code Organization & Architecture

Recommended project structure:

```
macro_regime_system/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ fred_md/                 # FRED-MD downloads
â”‚   â”œâ”€â”€ asset_prices/            # ETF prices
â”‚   â”œâ”€â”€ alfred_vintages/         # Real-time data
â”‚   â””â”€â”€ processed/               # Cached features
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ preprocessing/
â”‚   â”‚   â”œâ”€â”€ data_loader.py
â”‚   â”‚   â”œâ”€â”€ stationarity.py
â”‚   â”‚   â””â”€â”€ transformations.py
â”‚   â”œâ”€â”€ feature_engineering/
â”‚   â”‚   â”œâ”€â”€ ratios.py
â”‚   â”‚   â”œâ”€â”€ quintiles.py
â”‚   â”‚   â”œâ”€â”€ cointegration.py
â”‚   â”‚   â”œâ”€â”€ momentum.py
â”‚   â”‚   â””â”€â”€ hierarchical_clustering.py
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ linear_models.py
â”‚   â”‚   â”œâ”€â”€ tree_models.py
â”‚   â”‚   â””â”€â”€ neural_nets.py
â”‚   â”œâ”€â”€ evaluation/
â”‚   â”‚   â”œâ”€â”€ cross_validation.py
â”‚   â”‚   â”œâ”€â”€ metrics.py
â”‚   â”‚   â”œâ”€â”€ inference.py
â”‚   â”‚   â””â”€â”€ shap_analysis.py
â”‚   â””â”€â”€ visualization/
â”‚       â”œâ”€â”€ plotting.py
â”‚       â””â”€â”€ dashboards.py
â”œâ”€â”€ streamlit_app/
â”‚   â”œâ”€â”€ app.py
â”‚   â”œâ”€â”€ pages/
â”‚   â”‚   â”œâ”€â”€ 1_Experiment_Config.py
â”‚   â”‚   â”œâ”€â”€ 2_Model_Comparison.py
â”‚   â”‚   â”œâ”€â”€ 3_Feature_Importance.py
â”‚   â”‚   â”œâ”€â”€ 4_Dominant_Drivers.py
â”‚   â”‚   â””â”€â”€ 5_Data_Inspector.py
â”‚   â””â”€â”€ utils/
â”‚       â”œâ”€â”€ data_loader.py
â”‚       â””â”€â”€ backend.py
â”œâ”€â”€ experiments/
â”‚   â”œâ”€â”€ features/
â”‚   â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ cv_results/
â”‚   â””â”€â”€ shap/
â”œâ”€â”€ configs/
â”‚   â””â”€â”€ experiment_config.yaml
â””â”€â”€ run_tournament.py
```

---

## Computational Timing & Optimization

| Component | Per Asset | Ã— 3 Assets | Parallelized |
|---|---|---|---|
| Linear Models | 10â€“15 min | 30â€“45 min | 15â€“20 min |
| Tree Models | 30â€“45 min | 90â€“135 min | 35â€“50 min |
| Neural Networks | 45â€“60 min | 135â€“180 min | 50â€“70 min |
| SHAP Computation | 20â€“30 min | 60â€“90 min | 25â€“35 min |
| Regime Analysis | 15â€“20 min | 45â€“60 min | 20â€“25 min |
| Report Generation | 5â€“10 min | 15â€“30 min | 10â€“15 min |
| **FULL TOURNAMENT** | **125â€“180 min** | **375â€“540 min** | **155â€“215 min** |

**Optimization Strategies:**

- **GPU acceleration:** LSTM training drops from 60 min to 15 min per asset
- **Caching:** Load pre-computed features, avoid re-transformation
- **Incremental learning:** Monthly updates use existing models, quarterly full retraining
- **Smart hyperparameter init:** Use previous best as Optuna starting point

---

## Deployment Workflow & Checklist

### 5.1 Initial Setup (Week 1)

- [ ] Install dependencies: pandas, numpy, scikit-learn, xgboost, shap, streamlit
- [ ] Download FRED-MD current vintage
- [ ] Extend asset returns to 1959
- [ ] Run feature engineering pipeline (1.5â€“2h)
- [ ] Run full model tournament (6â€“9h)
- [ ] Generate initial reports

### 5.2 ALFRED Validation (Week 2)

- [ ] Download 48 ALFRED vintages (2000â€“2024)
- [ ] Run validation workflow (~20h, parallelizable to 4â€“5h)
- [ ] Compute revision risk metrics
- [ ] Verify deployment criteria met (IC > 0.15, Risk < 30%)
- [ ] Document feature stability

### 5.3 Production Deployment (Week 3)

- [ ] Launch Streamlit app: `streamlit run app.py`
- [ ] Configure monthly data update schedule
- [ ] Set up monitoring alerts (features in extreme ranges)
- [ ] Train team on monitoring sheet interpretation
- [ ] Document handoff procedures

### 5.4 Ongoing Maintenance

- **Monthly (30â€“40 min):** Download new FRED-MD + ETF prices, update features, generate forecasts
- **Quarterly (3â€“4 hours):** Re-train models with expanded data, update SHAP values
- **Annually:** Full ALFRED validation, review deployment criteria, update methodology if needed

---

## Final Deliverables Checklist

- [ ] Automated model tournament scripts
- [ ] Feature engineering pipeline (250â€“300 cluster representatives)
- [ ] Dominant driver reports (top 5â€“10 per asset with SHAP values)
- [ ] Interactive Streamlit dashboard (5 pages)
- [ ] ALFRED validation report (revision risk, feature stability)
- [ ] Production model registry (versioned, reproducible)
- [ ] Real-time monitoring sheets (current regime + signals)
- [ ] User documentation (methodology, interpretation guide)
- [ ] Code repository with README and examples
- [ ] Deployment guide for IT team

---

*End of Technical Appendix C*

---

*End of Document Suite*
